
# Inside TensorFlow 2.0

From [here](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/eager/function.py#L2224-L2227):

> Executing a graph generated by `defun` respects device annotations (i.e., all `with tf.device` directives present in a Python function will also be present in its corresponding graph), but it is not yet possible to execute the generated graphs across multiple machines.

From [here](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/eager/function.py#L2289-L2296):

> When using `defun`, there are subtleties regarding inputs, Python control flow, and variable creation that one should be aware of. For concreteness, let `f` be a Python function that returns zero or more `tf.Tensor` objects and let `F = defun(f)`. `F` builds a graph for each unique input signature it sees, Python control flow is baked into graphs, and operations related to variable initialization are automatically lifted out of the graphs that `F` generates and placed in the eager context if executing eagerly or into an outer graph otherwise.

From [here](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/eager/function.py#L2474-L2486):

> Finally, because each input signature is bound to a unique graph, if your Python function constructs `tf.Variable` objects, then each graph constructed for that Python function will reference a unique set of variables. To circumvent this problem, we recommend against compiling Python functions that create `tf.Variable` objects. Instead, Python functions should either lexically close over `tf.Variable` objects or accept them as arguments, preferably encapsulated in an object-oriented container. If you must create variables inside your Python function and you want each graph generated for it to reference the same set of variables, add logic to your Python function that ensures that variables are only created the first time it is called and are reused for every subsequent invocation; note that this is precisely what `tf.keras.layers.Layer` objects do, so we recommend using them to represent variable-bearing computations whenever possible.

`func_graph_from_py_func` is where the [defun magic](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/framework/func_graph.py#L915) happens.

## `tf.function`

To get graph def from function:

graph_def = f.get_concrete_function().graph.as_graph_def()

## Variables

There are several new [variable](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/ops/variables.py) types and a new notions

- [VariableSynchronization](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/ops/variables.py#L71).

- [VariableAggregation](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/ops/variables.py#L93).

- [VariableMetaclass](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/ops/variables.py#L177) *to allow construction of tf.Variable to be overridden*.

- [AbstractVariable](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/ops/variables.py#L3389) used e.g. by [DistributedVariable](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/distribute/values.py#L597).

## Eager Tensors

Store in (remote) memory.

- [EagerTensor](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/framework/ops.py#L1121) and [_EagerTensorBase](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/framework/ops.py#L855) with [_backing_device](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/framework/ops.py#L937-L945).
