# TensorFlow 2.0

| Status        | Draft |
:-------------- |:---------------------------------------------------- |
| **Author(s)** | Morten Dahl (mortendahlcs@gmail.com), Justin Patriquin (justin@dropoutlabs.com) |
| **Sponsor**   | |
| **Updated**   | 1029-10-22 |

## Objective

Support new paradigms and functionality:
- eager mode
- tf.function
- Keras

Adapt API of TFE to match:
- eager by default
- tfe.computation
- tfe.Variable

Other (this is a good a time as any):
- long overdue refactoring
- formalize interoperatability with crypto libs
- between-graph execution
- compilation to serializable components
- clarify TFE concepts and how users+devs should think

Benefits:
- user familiarity
- confidence in security guarantees

Outline of process:
- phase 1: minimal changes to work on TF2
- phase 2:

We want to integrate and take into account the major paradigm changes the TensorFlow (TF) team has made between TensorFlow 1.X (TF1) and 2.0 (TF2). These include [eager execution](https://www.tensorflow.org/guide/eager), the introduction of [tf.function](https://www.tensorflow.org/api_docs/python/tf/function) and AutoGraph, and encouraging Keras use for high-level APIs over everything else. This moves the focus from large graphs that are run all at once to a bunch of smaller graphs that are stitched together via eager execution.

This proposal is based on [release 2.0](https://github.com/tensorflow/tensorflow/tree/r2.0) of TensorFlow.

## Motivation

Supporting TF2 is important because it is the future of machine learning and there are important lessons to be learned by taking the paradigms introduced by TensorFlow 2.0 into account. Reading up on the paradigm changes is important to understand the direction that machine learning is going and will also help understand why certain design decisions were made in regards to TF Encrypted.

Both users and developers of TF Encrypted will benefit from integrating with TensorFlow 2.0. Users will benefit because they will be expecting APIs that work similarly to TensorFlow while gaining all of the benefits of the new paradigms introduced by 2.0. For example, the debuggability of models provided by eager execution will become a first class citizen for both data scientists building encrypted models and researchers building new secure computation protocols and integrations. Developers will also benefit from the debuggability and will be able to learn interesting lessons by deeply understanding the changes in TensorFlow 2.0.

## Design Proposal

TFE is about generating tfe.Computations, in the same way that TF can be thought of as being about generating tf.SavedModels and TFF about generating tff.FederatedComputations.

As in all distributed computations we first need some way of referring to the participating players. At this point these need not be linked to any running servers but are used to specify placements.

### Simple Use

```python
prediction_client, model_owner = setup_context()


with prediction_client:
  x_remote = tf.random.normal(some_shape)
  x = tfe.encrypt(x_remote)


with model_owner:
  w_remote = tf.random.normal(some_shape)
  w = tfe.Variable(w_remote)  # implicit encrypt due to protocol


# perform encrypted computation
y = tfe.matmul(x, w)


with prediction_client:

  # decrypt result for prediction client
  y_remote = tfe.decrypt(y)
  assert type(y_remote) == tfe.PrivateTensor
  assert y_remote.placement == {prediction_client}
  assert y_remote.sensitivity == {prediction_client}

  # result can be printed directly but will trigger a warning;
  # this can be avoided by first converting to PublicTensor
  tf.print(y_remote)
```

Implicit script runner player?

### Defining Computations

```python
@tfe.computation
def forward(x, w):
  # TODO(Morten) use Keras model here instead,
  # with weights updates according to input
  return tfe.matmul(x, w)

assert type(forward) == tfe.Computation
assert forward.protocol is None
assert forward.input_signature == None
assert forward.output_signature == None
```

This gives us an abstract `tfe.Computation` expressing *what* we want to compute but not yet saying anything about *how* it should be executed (the protocol is missing). To get the latter we need to turn it into a `tfe.ConcreteComputation` by either specifying an input signature and a protocol, or by invoking it in an execution context as done below. If needed then this decision can be defer to elsewhere since abstract computations are also serializable. Obtaining concrete computations is done via tracing and under the constraints it imposes.

Note that in the future we might be able to use ordinary TensorFlow, and  i.e replace `tfe.matmul` with `tf.matmul`. To obtain an abstract representation we might be able to synthesize ordinary tensors for the inputs based on their shape, trace the function using ordinary TensorFlow on these, and finally apply the converter to the generated graph.

We next define a computation for generating the prediction client's input locally on its machine. We use a `tfe.LocalComputation` for this, a subclass of `tfe.Computation` that fixes the protocol to `tfe.protocols.Plaintext` and allows us to use ordinary TensorFlow by wrapping it and lifting the result into TFE types. It is also useful for lifting values that have already been generated into TFE.

```python
@tfe.computation
def provide_input():
  x = # ... ordinary TensorFlow code ...
  return x

assert type(provide_input) == tfe.Computation
assert provide_input.protocol == None
assert provide_input.input_signature == None
assert provide_input.output_signature == None
```

### Executing Computations

```python
# specify how we want to represent, store, and compute on encrypted values;
# Pond will additive secret share values between the two server and use
# the third for triple generation
encrypted_protocol = tfe.protocols.Pond(server0, server1, server2)

# we also want some values to be processed in plaintext, yet
# we only want them to be revealed to the two servers; without
# this constraint we might as well perform the prediction locally
# on the prediction client;
# this protocol will replicate and process them on both servers
# to avoid network overhead
weights_protocol = tfe.protocols.Private({server0, server1})
```

```python
with weights_protocol:
  w = tfe.Variable()
```





Note that this could also have been expressed as follows (although here with a fixed and predefined number of outputs):

 using ordinary TensorFlow. This concept is similar to `tff.tf_computation`. 


 using ordinary TensorFlow. This concept is similar to `tff.tf_computation`. In some cases this would be done elsewhere


Once we have a computation w can either serialize it as an ordinary TF graph using `provide_input.as_graph_def()` or we can call it in our current execution context.

```python
# run computation on inputter and obtain remote reference
x_local = provide_input()

assert type(x_local) == tfe.PlaintextTensor
assert x_local.dtype == tfe.float32  # same as tf.float32
assert x_local.placement == {inputter}
assert type(x_local.protocol) == tfe.protocols.Plaintext
```

```python
# players only use for execution
server0 = tfe.Player('server0')
server1 = tfe.Player('server1')
server2 = tfe.Player('server2')
```

### Annotating Computations

In the above code we used the generic `tfe.computation` for everything, putting no constraints on tensor types and protocols. However, in larger systems it can become hard to keep track of how each computation is executed and which players sees which values in plaintext. TFE has several constructs to help mitigate these issues, making it easier to read scripts as well as catch potential security errors early on via type checking.

For instance, `provide_input` can instead be defined as a `tfe.PrivateComputation`:

```python
@tfe.private_computation(prediction_client)
def provide_input():
  x = # ... ordinary TensorFlow code ...
  return x

assert type(provide_input) == tfe.PrivateComputation
assert type(provide_input.protocol) == tfe.protocols.Private
assert provide_input.input_signature == None
assert provide_input.output_signature == None
```

This fixes the protocol to `tfe.protocols.Private`, and makes sure all executions and computed state is stored on the prediction client. Outputs can be used as inputs to other private computations by the same player, but an error is raised if these values are ever revealed in unencrypted form. It is roughly equivalent to the following (although it will not fix the number of outputs):

```python
@tfe.computation(
    protocol=tfe.protocols.Private(prediction_client),
    output_signature=[
        tfe.tensors.PrivateTensor.with_placement(prediction_client)
    ]
)
def provide_input():
  x = # ... ordinary TensorFlow code ...
  return ordinary_tf_code()
```

Similarly, it can be useful to make it explicit that `forward` is supposed to be an encrypted computation, and make sure that an error is raised if it is ever invoked with plaintext values.

```python
@tfe.encrypted_computation
def forward(x, w):
  return tfe.matmul(x, w)
```

The above is roughly equivalent to the following (again without fixing the number of inputs and outputs):

```python
@tfe.computation(
    protocol=tfe.protocols.Encrypted,
    input_signature={
        'x': tfe.tensors.EncryptedTensor,
        'w': tfe.tensors.EncryptedTensor,
    },
    output_signature=[
        tfe.tensors.EncryptedTensor
    ]
)
def forward(x, w):
  return tfe.matmul(x, w)
```

Note that this may in be too specific, for instance preventing us from using a private `w`. We can loosening these constraints as follows, allowing private values with the specific placement:

```python
@tfe.encrypted_computation(
    input_signature={
        'w': tfe.tensors.PrivateTensor.with_placement({server0, server1})
    }
)
def forward(x, w):
  return tfe.matmul(x, w)
```

Below we will use these protocol as execution contexts, however we may omit this by setting a global current protocol using `tfe.set_protocol`.

Local computations are always executed as instance of the plaintext protocol on the specified device, independently of the current execution context.

```python
# encrypt x on the inputter, and send result to the servers
with encrypted_protocol:
  x_encrypted = tfe.encrypt(x_local)

assert type(x_encrypted) is tfe.TwoPartyAdditiveTensor
assert x_encrypted.dtype is tfe.fixed32
assert x_encrypted.placement is (server0, server1)
assert type(x_encrypted.protocol) is tfe.protocols.Pond
```

`tfe.encrypt` is an abstract `tfe.Computation` which here is made concrete per the type of `x_local` and `encrypted_protocol`. The `encrypt` kernel registered by the `Pond` protocol for local tensors will pick up the placement of the value and perform the encryption here.

```python
tfe.set_protocol(encrypted_protocol)
```

```python

# use current execution context to allocate
# variable matching with current protocol
v = tfe.Variable(tf.fill(shape, value=5))
```


### tfe.computation

- why computation and not function
- no global state and fix for memoization

composition

Inside of Pond we use memoization to help gain some runtime efficiencies by caching already calculated results. When used in conjunction with Variables and wrapping them in a class this causes graph tensors to be leaked and then passed as inputs to tf.function when it is actually run. tf.functions are run in eager mode so it expects actual values not just graph tensors that do not yet contain values.

Leaking graph function also appears to be caused by calling read_value() on the variable when variable is initialized inside of Pond.

The following examples fails:

```python
class C: pass
obj = C(); obj.v = None

@tf.function
def g(x):
  if obj.v is None:
    obj.v = tfe.define_private_variable(x)
  return obj.v.reveal().to_native()
```

With this error:

```
TypeError: An op outside of the function building code is being passed
a "Graph" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: decode/truediv:0
```

Disabling memoization and not using read_value() fixes this problem.



### tfe.Variable

- `tfe.tensors.Variable`: abstract root; also exposed as `tfe.Tensor`.
  - `tfe.tensors.EncryptedVariable`
  - `tfe.tensors.PlaintextVariable`: abstract tensor holding plaintext values replicated across a number of players.
    - `tfe.tensors.PrivateTensor`: concrete `PlaintextTensor` representing values that are to be considered private; any attempt to move them to other players will raise an error.
    - `tfe.tensors.PublicTensor`: concrete `PlaintextTensor` representing values that can safely be shared with anyone, even potentially included as constants in serialized computations; difference is that sensitivity is None.


TODO

This is the meat of the document, where you explain your proposal. If you have multiple alternatives, be sure to use sub-sections for better separation of the idea, and list pros/cons to each approach. If there are alternatives that you have eliminated, you should also list those here, and explain why you believe your chosen approach is superior.

Factors to consider include:

performance implications
dependencies
maintenance
platforms and environments impacted (e.g. hardware, cloud, other software ecosystems)
how will this change impact users, and how will that be managed?

## Detailed Design Proposal

### Types and Type Checking

#### Computations

- `tfe.Computation`
  - `tfe.EncryptedComputation`
  - `tfe.PrivateComputation`

- `tfe.ConcreteComputation`

#### Tensors

- `tfe.tensors.Tensor`: abstract root; also exposed as `tfe.Tensor`.
  - `tfe.tensors.EncryptedTensor`
  - `tfe.tensors.PlaintextTensor`: abstract tensor holding plaintext values replicated across a number of players.
    - `tfe.tensors.PrivateTensor`: concrete `PlaintextTensor` representing values that are to be considered private; any attempt to move them to other players will raise an error.
    - `tfe.tensors.PublicTensor`: concrete `PlaintextTensor` representing values that can safely be shared with anyone, even potentially included as constants in serialized computations.

remove private and public, and make it a property on plaintext instead (determined by whether sensitivity is None)?

which all have properties:

- placement: set of players where (parts of) values reside.
- sensitivity: set of players who are allowed to see the values; `None` is unrestricted (ie public?).
- dtype: `tfe.fixed16`, `tfe.fixed32`, `tfe.int16`, etc.
- shape
- protocol: need this? values should be somewhat independent of protocol.

and all have class methods:

- `with_placement`

`tfe.tensors.EncryptedTensor` is the root of another hierarchy partially defined by the available protocols. :

- `tfe.tensors.EncryptedTensor`
  - `tfe.tensors.AdditiveTensor`
    - `tfe.tensors.TwoPlayerAdditiveTensor`: specific case of `AdditiveTensor` used by e.g. Pond and SecureNN.
  - `tfe.tensors.ReplicatedTensor`
    - `tfe.tensors.ThreePlayerReplicatedTensor`
  - `tfe.tensors.SealTensor`
  - `tfe.tensors.TrustedTensor`

Note that some of these tensors are independent of the available protocols and can be reused across them; this is because several protocols typically compute on data represented the same way. This ensures a separation between the data and how it's computed on.

#### Subtyping

We say that `x` is a subtype of `y` if:

- `isinstance(x, y)` is true;
- and `x.placement == y.placement`;
- and `x.sensitivity <= y.sensitivity`;
- and `x.protocol

for private tensors, placement set can implicitly be reduced to match types, but only enlarged through explicit TFE operation (which? tfe.reveal? tfe.share? tfe.move?)

#### Protocols

Class hierarchy with `tfe.protocols.Protocol` at the root:

- `tfe.protocols.Encrypted`
  - `tfe.protocols.Pond`
    - `tfe.protocols.SecureNN`
- `tfe.protocols.Plaintext`
  - `tfe.protocols.Private`
  - `tfe.protocols.Public`


## Questions and Discussion Topics

TODO

Seed this with open questions you require feedback on from the RFC process.

### Function annotations

On the need for this:

- Gather operations:
  - Pond can optimize triples for complex computation (like what we’re doing now for the entire computation)
  - SEAL/CHET can optimize HE parameters, including value packing, multiplication depth, and precision

- encrypted computations could be made serializable and run in another context, potentially non-Python based (similar to federated computations in TFF)

- encrypted computations can be generic wrt the underlying protocol, and only made concrete when called, making the how independent from the what

- via the convert we could potentially support the use of native TF operations when specifying encrypted computation instead of insisting on TFE operations
- they are strictly speaking not needed: g could have been unannotated in the above (edited)
- verify confidntiality under security assumptions
- confidentiality clear from script code

- `tfe.encrypted_computation`
- `tfe.plaintext_computation`
- `tfe.function`

Key questions:
- will this be liked from a user's perspective?
- will this fit together with TF2, PySyft, TFF?

another issue: should we simply use tfe.function instead of local_computation, encrypted_computation etc?

problem is that there are so many combinations between the input/output of these guys, with no good name that represents all (edited) 

and the big question is: what kind of annotations do we need in the code to help users as much as possible

signature of function is something like `(type(x1), ..., type(xn)) X type(ops/prot) X (type(y1), ..., type(ym))`

a function that computes on mixed type inputs and mixed type outputs, what is that? is an encrypted computations just a special case of functions? might be nice to make it clear that secure aggregation is an encrypted computation (from plaintext to plaintexts) that never reveals any values
- use sensitivity of input to determine whether or not to automatically encrypt and decrypt? "sensitivity" could be specified by whom allowed to see the value (instead of just binary)

- maybe only traced (concrete) computations can be encrypted? ie it depends on the protocol used?

encrypted or not is determined by protocol (given that we have a plaintext protocol)

### Variables

should we use tfe.EncryptedVariable and tfe.PlaintextVariable?
maybe they come in addition to tfe.Variable
nvm, i’ll stick with tfe.Variable for now

Pond for instance supports two kinds of variables, but maybe it shouldn’t — and public variables should be a different but related protocol (that stores data close to the servers used in pond)
