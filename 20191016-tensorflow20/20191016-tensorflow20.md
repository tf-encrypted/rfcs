# TensorFlow 2.0

## Objective

We want to integrate with TensorFlow 2.0 where we take into account the major paradigm changes the TensorFlow team made between TensorFlow 1.X and 2.0. These include the introduction of tf.function and AutoGraph, eager execution and encouraging Keras use for high-level APIs over everything else. While the fundamental idea of constructing graphs to improve runtime efficiency how these graphs are created and then stitched together create some interesting problems and opportunities. This moves the focus from large graphs that are run all at once to a bunch of smaller graphs that are stitched together via eager execution.

## Motivation

Supporting TensorFlow 2.0 is important because it is the future of machine learning and there are important lessons to be learned by taking the paradigms introduced by TensorFlow 2.0 into account. Reading up on the paradigm changes is important to understand the direction that machine learning is going and will also help understand why certain design decisions were made in regards to TF Encrypted.

Both users and developers of TF Encrypted will benefit from integrating with TensorFlow 2.0. Users will benefit because they will be expecting APIs that work similarly to TensorFlow while gaining all of the benefits of the new paradigms introduced by 2.0. For example, the debuggability of models provided by eager execution will become a first class citizen for both data scientists building encrypted models and researchers building new secure computation protocols and integrations. Developers will also benefit from the debuggability and will be able to learn interesting lessons by deeply understanding the changes in TensorFlow 2.0.

## Known Problems

### Variable Initialization

When a function is decorated with tf.function decorator the function is automatically traced and can end up being called more than once. For this reason, variables must be proven not be initialized more than once. TensorFlow documentation recommends that you either create the variable outside of the function or wrap the variable in a class which can store the initialization of the Variable. Following example taken from here.

Create variable outside of function:

```python
v = tf.Variable(1.0)

@tf.function
def f(x):
  return v.assign_add(x)
```

Wrap variable in a class:

```python
class C: pass
obj = C(); obj.v = None

@tf.function
def g(x):
  if obj.v is None:
    obj.v = tf.Variable(1.0)
  return obj.v.assign_add(x)
```

These same principles can be applied when creating Keras models or optimizers inside of a `tf.function`.

### Memoization Causing Leak of Graph Tensors

Inside of Pond we use memoization to help gain some runtime efficiencies by caching already calculated results. When used in conjunction with Variables and wrapping them in a class this causes graph tensors to be leaked and then passed as inputs to tf.function when it is actually run. tf.functions are run in eager mode so it expects actual values not just graph tensors that do not yet contain values.

Leaking graph function also appears to be caused by calling read_value() on the variable when variable is initialized inside of Pond.

The following examples fails:

```python
class C: pass
obj = C(); obj.v = None

@tf.function
def g(x):
  if obj.v is None:
    obj.v = tfe.define_private_variable(x)
  return obj.v.reveal().to_native()
```

With this error:

```
TypeError: An op outside of the function building code is being passed
a "Graph" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: decode/truediv:0
```

Disabling memoization and not using read_value() fixes this problem.

## Design Proposal

TODO

This is the meat of the document, where you explain your proposal. If you have multiple alternatives, be sure to use sub-sections for better separation of the idea, and list pros/cons to each approach. If there are alternatives that you have eliminated, you should also list those here, and explain why you believe your chosen approach is superior.

Factors to consider include:

performance implications
dependencies
maintenance
platforms and environments impacted (e.g. hardware, cloud, other software ecosystems)
how will this change impact users, and how will that be managed?

## Detailed Design Proposal

TODO

This section is optional. Elaborate on details if they’re important to understanding the design, but would make it hard to read the proposal section above.

## Questions and Discussion Topics

TODO

Seed this with open questions you require feedback on from the RFC process.

### Function annotations

On the need for this:

- Gather operations:
  - Pond can optimize triples for complex computation (like what we’re doing now for the entire computation)
  - SEAL/CHET can optimize HE parameters, including value packing, multiplication depth, and precision

- encrypted computations could be made serializable and run in another context, potentially non-Python based (similar to federated computations in TFF)

- encrypted computations can be generic wrt the underlying protocol, and only made concrete when called, making the how independent from the what

- via the convert we could potentially support the use of native TF operations when specifying encrypted computation instead of insisting on TFE operations
- they are strictly speaking not needed: g could have been unannotated in the above (edited)
- verify confidntiality under security assumptions
- confidentiality clear from script code

- `tfe.encrypted_computation`
- `tfe.plaintext_computation`
- `tfe.function`

Key questions:
- will this be liked from a user's perspective?
- will this fit together with TF2, PySyft, TFF?

another issue: should we simply use tfe.function instead of local_computation, encrypted_computation etc?

problem is that there are so many combinations between the input/output of these guys, with no good name that represents all (edited) 

and the big question is: what kind of annotations do we need in the code to help users as much as possible

signature of function is something like `(type(x1), ..., type(xn)) X type(ops/prot) X (type(y1), ..., type(ym))`

a function that computes on mixed type inputs and mixed type outputs, what is that? is an encrypted computations just a special case of functions? might be nice to make it clear that secure aggregation is an encrypted computation (from plaintext to plaintexts) that never reveals any values
- use sensitivity of input to determine whether or not to automatically encrypt and decrypt? "sensitivity" could be specified by whom allowed to see the value (instead of just binary)

- maybe only traced (concrete) computations can be encrypted? ie it depends on the protocol used?

encrypted or not is determined by protocol (given that we have a plaintext protocol)

### Variables

should we use tfe.EncryptedVariable and tfe.PlaintextVariable?
maybe they come in addition to tfe.Variable
nvm, i’ll stick with tfe.Variable for now

Pond for instance supports two kinds of variables, but maybe it shouldn’t — and public variables should be a different but related protocol (that stores data close to the servers used in pond)


## Appendix: Inside TensorFlow 2.0

From [here](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/eager/function.py#L2224-L2227):

> Executing a graph generated by `defun` respects device annotations (i.e., all `with tf.device` directives present in a Python function will also be present in its corresponding graph), but it is not yet possible to execute the generated graphs across multiple machines.

From [here](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/eager/function.py#L2289-L2296):

> When using `defun`, there are subtleties regarding inputs, Python control flow, and variable creation that one should be aware of. For concreteness, let `f` be a Python function that returns zero or more `tf.Tensor` objects and let `F = defun(f)`. `F` builds a graph for each unique input signature it sees, Python control flow is baked into graphs, and operations related to variable initialization are automatically lifted out of the graphs that `F` generates and placed in the eager context if executing eagerly or into an outer graph otherwise.

From [here](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/eager/function.py#L2474-L2486):

> Finally, because each input signature is bound to a unique graph, if your Python function constructs `tf.Variable` objects, then each graph constructed for that Python function will reference a unique set of variables. To circumvent this problem, we recommend against compiling Python functions that create `tf.Variable` objects. Instead, Python functions should either lexically close over `tf.Variable` objects or accept them as arguments, preferably encapsulated in an object-oriented container. If you must create variables inside your Python function and you want each graph generated for it to reference the same set of variables, add logic to your Python function that ensures that variables are only created the first time it is called and are reused for every subsequent invocation; note that this is precisely what `tf.keras.layers.Layer` objects do, so we recommend using them to represent variable-bearing computations whenever possible.

`func_graph_from_py_func` is where the [defun magic](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/framework/func_graph.py#L915) happens.

### `tf.function`

To get graph def from function:

graph_def = f.get_concrete_function().graph.as_graph_def()

### Variables

There are several new [variable](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/ops/variables.py) types and a new notions

- [VariableSynchronization](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/ops/variables.py#L71).

- [VariableAggregation](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/ops/variables.py#L93).

- [VariableMetaclass](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/ops/variables.py#L177) *to allow construction of tf.Variable to be overridden*.

- [AbstractVariable](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/ops/variables.py#L3389) used e.g. by [DistributedVariable](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/distribute/values.py#L597).

### Eager Tensors

Store in (remote) memory.

- [EagerTensor](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/framework/ops.py#L1121) and [_EagerTensorBase](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/framework/ops.py#L855) with [_backing_device](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/framework/ops.py#L937-L945).
