# Objective

We want to integrate with TensorFlow 2.0 where we take into account the major paradigm changes the TensorFlow team made between TensorFlow 1.X and 2.0. These include the introduction of tf.function and AutoGraph, eager execution and encouraging Keras use for high-level APIs over everything else. While the fundamental idea of constructing graphs to improve runtime efficiency how these graphs are created and then stitched together create some interesting problems and opportunities. This moves the focus from large graphs that are run all at once to a bunch of smaller graphs that are stitched together via eager execution.

# Motivation
Supporting TensorFlow 2.0 is important because it is the future of machine learning and there are important lessons to be learned by taking the paradigms introduced by TensorFlow 2.0 into account. Reading up on the paradigm changes is important to understand the direction that machine learning is going and will also help understand why certain design decisions were made in regards to TF Encrypted.

Both users and developers of TF Encrypted will benefit from integrating with TensorFlow 2.0. Users will benefit because they will be expecting APIs that work similarly to TensorFlow while gaining all of the benefits of the new paradigms introduced by 2.0. For example, the debuggability of models provided by eager execution will become a first class citizen for both data scientists building encrypted models and researchers building new secure computation protocols and integrations. Developers will also benefit from the debuggability and will be able to learn interesting lessons by deeply understanding the changes in TensorFlow 2.0.

# Known Problems

## Variable Initialization
When a function is decorated with tf.function decorator the function is automatically traced and can end up being called more than once. For this reason, variables must be proven not be initialized more than once. TensorFlow documentation recommends that you either create the variable outside of the function or wrap the variable in a class which can store the initialization of the Variable. Following example taken from here.

Create variable outside of function:

```
v = tf.Variable(1.0)

@tf.function
def f(x):
  return v.assign_add(x)
```

Wrap variable in a class:

```
class C: pass
obj = C(); obj.v = None

@tf.function
def g(x):
  if obj.v is None:
    obj.v = tf.Variable(1.0)
  return obj.v.assign_add(x)
```

These same principles can be applied when creating Keras models or optimizers inside of a `tf.function`.

## Memoization Causing Leak of Graph Tensors

Inside of Pond we use memoization to help gain some runtime efficiencies by caching already calculated results. When used in conjunction with Variables and wrapping them in a class this causes graph tensors to be leaked and then passed as inputs to tf.function when it is actually run. tf.functions are run in eager mode so it expects actual values not just graph tensors that do not yet contain values.

Leaking graph function also appears to be caused by calling read_value() on the variable when variable is initialized inside of Pond.

The following examples fails:

```
class C: pass
obj = C(); obj.v = None

@tf.function
def g(x):
  if obj.v is None:
    obj.v = tfe.define_private_variable(x)
  return obj.v.reveal().to_native()
```

With this error:

```
TypeError: An op outside of the function building code is being passed
a "Graph" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: decode/truediv:0
```

Disabling memoization and not using read_value() fixes this problem.

# Design Proposal

TODO

This is the meat of the document, where you explain your proposal. If you have multiple alternatives, be sure to use sub-sections for better separation of the idea, and list pros/cons to each approach. If there are alternatives that you have eliminated, you should also list those here, and explain why you believe your chosen approach is superior.

Factors to consider include:

performance implications
dependencies
maintenance
platforms and environments impacted (e.g. hardware, cloud, other software ecosystems)
how will this change impact users, and how will that be managed?

# Detailed Design

TODO

This section is optional. Elaborate on details if theyâ€™re important to understanding the design, but would make it hard to read the proposal section above.

# Questions and Discussion Topics

TODO

Seed this with open questions you require feedback on from the RFC process.

